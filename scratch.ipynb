{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\n",
    "    \"Voxel51/MedXpertQA\",\n",
    "    name=\"MedXpertQA\",\n",
    "    max_samples=10,\n",
    "    overwrite=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from typing import List, Dict, Any, Optional, Union, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "import fiftyone as fo\n",
    "from fiftyone import Model, SamplesMixin\n",
    "from fiftyone.core.labels import Classification, Classifications\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "\n",
    "DEFAULT_CLASSIFICATION_SYSTEM_PROMPT = \"\"\"You specialize in comprehensive classification across medical domains including radiology images, histopathology patches, ophthalmology images, and dermatology images.\n",
    "\n",
    "Unless specifically requested for single-class output, you may report multiple relevant classifications. Report all classifications as JSON array of predictions in the format: \n",
    "\n",
    "```json\n",
    "{\n",
    "    \"classifications\": [\n",
    "        {\n",
    "            \"label\": \"descriptive medical condition or relevant label\",\n",
    "            \"label\": \"descriptive medical condition or relevant label\",\n",
    "            ...,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "Always return your response as valid JSON wrapped in ```json blocks. \n",
    "\n",
    "You may report multiple lables if they are relevant. Do not report your confidence.\n",
    "\"\"\"\n",
    "\n",
    "DEFAULT_VQA_SYSTEM_PROMPT = \"\"\"You are an expert across medical domains including radiology images, histopathology patches, ophthalmology images,\n",
    "and dermatology images. You provide expert-level answers to medical questions.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "MEDGEMMA_OPERATIONS = {\n",
    "    \"vqa\": {\n",
    "        \"system_prompt\": DEFAULT_VQA_SYSTEM_PROMPT,\n",
    "    },\n",
    "    \"classify\": {\n",
    "        \"system_prompt\": DEFAULT_CLASSIFICATION_SYSTEM_PROMPT,\n",
    "    }\n",
    "}\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Utility functions\n",
    "def get_device():\n",
    "    \"\"\"Get the appropriate device for model inference.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "class medgemma(SamplesMixin, Model):\n",
    "    \"\"\"A FiftyOne model for running Gemma3 vision tasks\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        operation: str = None,\n",
    "        prompt: str = None,\n",
    "        system_prompt: str = None,\n",
    "        quantized: bool = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "        self._fields = {}\n",
    "        \n",
    "        self.model_path = model_path\n",
    "        self._custom_system_prompt = system_prompt  # Store custom system prompt if provided\n",
    "        self._operation = operation\n",
    "        self.prompt = prompt\n",
    "        self.quantized = quantized\n",
    "        \n",
    "        self.device = get_device()\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Set dtype for CUDA devices\n",
    "        self.torch_dtype = torch.bfloat16 if self.device == \"cuda\" else \"auto\"\n",
    "        \n",
    "        # Load model and processor\n",
    "        logger.info(f\"Loading model from {model_path}\")\n",
    "\n",
    "        model_kwargs = {\n",
    "            \"trust_remote_code\": True,\n",
    "            \"device_map\": self.device,\n",
    "            \"torch_dtype\":self.torch_dtype \n",
    "        }\n",
    "\n",
    "        if self.quantized:\n",
    "            model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "        self.model = AutoModelForImageTextToText.from_pretrained(\n",
    "            model_path,\n",
    "            **model_kwargs\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Loading processor\")\n",
    "\n",
    "        self.processor = AutoProcessor.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            use_fast=True\n",
    "        )\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def _get_field(self):\n",
    "        if \"prompt_field\" in self.needs_fields:\n",
    "            prompt_field = self.needs_fields[\"prompt_field\"]\n",
    "        else:\n",
    "            prompt_field = next(iter(self.needs_fields.values()), None)\n",
    "\n",
    "        return prompt_field\n",
    "\n",
    "    @property\n",
    "    def media_type(self):\n",
    "        return \"image\"\n",
    "    \n",
    "    @property\n",
    "    def operation(self):\n",
    "        return self._operation\n",
    "\n",
    "    @operation.setter\n",
    "    def operation(self, value):\n",
    "        if value not in MEDGEMMA_OPERATIONS:\n",
    "            raise ValueError(f\"Invalid operation: {value}. Must be one of {list(MEDGEMMA_OPERATIONS.keys())}\")\n",
    "        self._operation = value\n",
    "\n",
    "    @property\n",
    "    def system_prompt(self):\n",
    "        # Return custom system prompt if set, otherwise return default for current operation\n",
    "        return self._custom_system_prompt if self._custom_system_prompt is not None else MEDGEMMA_OPERATIONS[self.operation][\"system_prompt\"]\n",
    "\n",
    "    @system_prompt.setter\n",
    "    def system_prompt(self, value):\n",
    "        self._custom_system_prompt = value\n",
    "\n",
    "    def _parse_json(self, s: str) -> Optional[Dict]:\n",
    "        \"\"\"Parse JSON from model output.\n",
    "        \n",
    "        Args:\n",
    "            s: String output from the model to parse\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Parsed JSON dictionary if successful\n",
    "            None: If parsing fails or input is invalid\n",
    "            Original input: If input is not a string\n",
    "        \"\"\"\n",
    "        if not isinstance(s, str):\n",
    "            return s\n",
    "            \n",
    "        # Handle JSON wrapped in markdown code blocks\n",
    "        if \"```json\" in s:\n",
    "            try:\n",
    "                s = s.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "            except IndexError:\n",
    "                logger.debug(\"Failed to extract JSON from markdown blocks\")\n",
    "                return None\n",
    "        \n",
    "        try:\n",
    "            return json.loads(s)\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.debug(f\"JSON parse error: {e}. First 200 chars: {s[:200]}\")\n",
    "            return None\n",
    "\n",
    "    def _to_classifications(self, data: Dict) -> fo.Classifications:\n",
    "        \"\"\"Convert JSON classification data to FiftyOne Classifications.\n",
    "        \n",
    "        Args:\n",
    "            data: Dictionary containing a 'classifications' list where each item has:\n",
    "                - 'label': String class label\n",
    "            \n",
    "        Returns:\n",
    "            fo.Classifications object containing the converted classification annotations\n",
    "            \n",
    "        Example input:\n",
    "            {\n",
    "                \"classifications\": [\n",
    "                    {\"label\": \"condition_1\"},\n",
    "                    {\"label\": \"condition_2\"}\n",
    "                ]\n",
    "            }\n",
    "        \"\"\"\n",
    "        classifications = []\n",
    "        \n",
    "        try:\n",
    "            # Extract the classifications list from the input dictionary\n",
    "            classes = data.get(\"classifications\", [])\n",
    "            \n",
    "            # Process each classification dictionary\n",
    "            for cls in classes:\n",
    "                try:\n",
    "                    if not isinstance(cls, dict) or \"label\" not in cls:\n",
    "                        logger.debug(f\"Invalid classification format: {cls}\")\n",
    "                        continue\n",
    "                        \n",
    "                    classification = fo.Classification(\n",
    "                        label=str(cls[\"label\"]),\n",
    "                    )\n",
    "                    classifications.append(classification)\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Error processing classification {cls}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error processing classifications data: {e}\")\n",
    "            \n",
    "        return fo.Classifications(classifications=classifications)\n",
    "\n",
    "    def _predict(self, image: Image.Image, sample=None) -> Union[fo.Classifications, str]:\n",
    "        \"\"\"Process a single image through the model and return predictions.\"\"\"\n",
    "        if sample is not None and self._get_field() is not None:\n",
    "            field_value = sample.get_field(self._get_field())\n",
    "            if field_value is not None:\n",
    "                self.prompt = str(field_value)\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": self.system_prompt}\n",
    "                            ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": self.prompt},\n",
    "                    {\"type\": \"image\", \"image\": image}  # Pass the PIL Image directly\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        text = self.processor.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=True, \n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_dict=True,\n",
    "            ).to(self.device, dtype=self.torch_dtype)\n",
    "\n",
    "        input_len = text[\"input_ids\"].shape[-1]\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            generation = self.model.generate(\n",
    "                **text, \n",
    "                max_new_tokens=8192, \n",
    "                do_sample=False\n",
    "                )\n",
    "            generation = generation[0][input_len:]\n",
    "\n",
    "        output_text = self.processor.decode(generation, skip_special_tokens=True)\n",
    "\n",
    "        # For VQA, return the raw text output\n",
    "        if self.operation == \"vqa\":\n",
    "            return output_text.strip()\n",
    "\n",
    "        # For other operations, parse JSON and convert to appropriate format\n",
    "        parsed_output = self._parse_json(output_text)\n",
    "        if not parsed_output:\n",
    "            return None\n",
    "        \n",
    "        if self.operation == \"classify\":\n",
    "            return self._to_classifications(parsed_output)\n",
    "\n",
    "\n",
    "    def predict(self, image, sample=None):\n",
    "        \"\"\"Process an image with the model.\n",
    "        \n",
    "        A convenience wrapper around _predict that handles numpy array inputs\n",
    "        by converting them to PIL Images first.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image or numpy array to process\n",
    "            sample: Optional FiftyOne sample containing the image filepath\n",
    "            \n",
    "        Returns:\n",
    "            Model predictions in the appropriate format for the current operation\n",
    "        \"\"\"\n",
    "        if isinstance(image, np.ndarray):\n",
    "            image = Image.fromarray(image)\n",
    "        return self._predict(image, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = medgemma(\n",
    "    model_path=\"google/medgemma-4b-it\",\n",
    "    quantized=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_system_labels = dataset.distinct(\"body_system.label\")\n",
    "\n",
    "model.operation = \"classify\"\n",
    "\n",
    "model.prompt = \"As a medical expert your task is to classify this image into exactly one of the following body systems: \" + \", \".join(body_system_labels)\n",
    "\n",
    "dataset.apply_model(model, label_field=\"pred_body_system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.operation=\"classify\"\n",
    "\n",
    "model.system_prompt = \"\"\"You have expert-level medical knowledge in radiology, histopathology, ophthalmology, and dermatology.\n",
    "\n",
    "You are presented with the medical history of a patient and an accompanying image related to the patient. \n",
    "\n",
    "You will be asked a multiple question and are required to select from one of the available answers.\n",
    "\n",
    "Once you have the answer, you  must respond in the following format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"classifications\": [\n",
    "        {\n",
    "            \"label\": \"your single letter answer to the question\",\n",
    "            ...,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Always return your response as valid JSON wrapped in ```json blocks and respond only with a one letter answer (one of A, B, C, D, E).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "dataset.apply_model(\n",
    "    model, \n",
    "    label_field=\"model_answer\", \n",
    "    prompt_field=\"question\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.operation=\"vqa\"\n",
    "\n",
    "dataset.apply_model(model, label_field=\"generated_answer\", prompt_field=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fo_develop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
